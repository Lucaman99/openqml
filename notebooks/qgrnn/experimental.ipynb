{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Graph Reccurent Neural Network in Pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts by importing all of the necessary dependencies\n",
    "\n",
    "import pennylane as qml\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "import random\n",
    "import math\n",
    "import networkx as nx\n",
    "import seaborn\n",
    "import timeit\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will investigate the idea of a **quantum graph neural network**, which is the quantum analogue of a classical graph neural network. More specicically, the quantum neural network we will be investigating is a **recurrent** quantum graph neural network. We will disucss what makes this sublcass unique later in the Notebook, but for now, we will note that these QGRNNs are very useful for learning the dynamics of quantum systems that \"live\" on a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Quantum Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, the idea of a quantum graph neural network has been receving a lot of attention from the machine learning research community for its ability to learn representations of data that is inhernelty graph-theoretic in nature. More specifically, graph neural networks seek to learn a **representation** (a mapping of the data into a lower-dimensional vector space) of a given graph, with features assigned to nodes and edges, such that the each of the vectors in the learned representation preserves the overall topology of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attempt to modify the same task, but from a quantum computational perspective. Specifically, we want to define an ansatz that we can use for quantum machine learning tasks that are inherently graph-theoretic. From the original QGNN paper, the general QGNN ansatz is defined as:\n",
    "\n",
    "<br>\n",
    "$$U(\\boldsymbol\\gamma, \\ \\boldsymbol\\theta) \\ = \\ \\displaystyle\\prod_{i \\ = \\ 1}^{P} \\Bigg[ \\displaystyle\\prod_{j \\ = \\ 1}^{Q} e^{-i \\gamma_{ij} H_{j}(\\boldsymbol\\theta) }\\Bigg]$$\n",
    "<br>\n",
    "\n",
    "Where we have:\n",
    "\n",
    "<br>\n",
    "$$\\hat{H}_{j}(\\boldsymbol\\theta) \\ = \\ \\displaystyle\\sum_{(a,b) \\in E} \\displaystyle\\sum_{c} V_{jabc} \\hat{A}_{a}^{jc} \\otimes \\hat{B}_{b}^{jc} \\ + \\ \\displaystyle\\sum_{v \\in V} \\displaystyle\\sum_{d} W_{jvd} \\hat{C}_{v}^{jd}$$\n",
    "<br>\n",
    "\n",
    "As you can see, this is the class of Hamiltonians that posses a direct mapping between interaction and bias terms, and the edges and vertices (repsectively) of some graph $G \\ = \\ (V, \\ E)$. This form of the Hamiltonian is very general, and as a result, has a lot of indices. The subscripts on operators represent some operator acting on the $a$-th, $b$-th or $v$-th qubit, and the other parameters ($c$ and $d$) simply allow us to label each parameter/type of operator in each term of both sums. This ansatz is fairly general: all it essentially tells us is that the QGNN involves many layers of parametrized operators that act on qubits according to the structure of some graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Quantum Graph Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have discussed what a general quantum graph neural network looks like, let's discuss what distinguishes a quantum graph RNN from its more general counterpart. With a graph RNN, we make one small change to the $\\boldsymbol\\gamma$ parameters. Specifically, we tie these parameters over temproal layers, meaning that each $\\gamma_{ij}$ is only determined by the index $j$, and is the same across all values of $i$ (with some fixed $j$). As the paper explains, this is equivalent to how a classical recurrent neural network behaves, where parameters remain the same over layers of the neural network. Thus, we will have:\n",
    "\n",
    "<br>\n",
    "$$U_{\\text{RNN}}(\\gamma, \\ \\theta) \\ = \\ \\displaystyle\\prod_{i \\ = \\ 1}^{P} \\Bigg[ \\displaystyle\\prod_{j \\ = \\ 1}^{Q} e^{-i \\gamma_{j} H_{j}(\\boldsymbol\\theta) }\\Bigg]$$\n",
    "<br>\n",
    "\n",
    "Now, the Trotter-Suzuki decomposition says that:\n",
    "\n",
    "<br>\n",
    "$$\\exp \\Bigg[ \\displaystyle\\sum_{n} A_n \\Bigg] \\ = \\ \\lim_{P \\rightarrow \\infty} \\displaystyle\\prod_{j \\ = \\ 1}^{P} \\Bigg[ \\displaystyle\\prod_{n} e^{A_n / P} \\Bigg]$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where finite $P \\ \\gg \\ 1$ approximates the left-hand side of the equation. It isn't too difficult to see that the quantum graph RNN resembles the Trotter-Suzuki decomposition. More specifically, the quantum graph RNN can be thought of as the Trotterization of the time-evolution operator, for some Hamiltonian. Let us fix a time $T$. Let us also fix a parameter that controls the size of the Trotterization steps (essentially the $1/P$ in the above formula), which we call $\\Delta$. This allows uas to keep the precision of our approximate time-evolution for different values of $T$ the same. Let us define:\n",
    "\n",
    "<br>\n",
    "$$\\hat{H} \\ = \\ \\displaystyle\\sum_{q} \\hat{H}_{q}(\\boldsymbol\\theta)$$\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "By using the Trotter-Suzuki decomposition defined above, we can see that the time-evolution operator for this particular Hamiltonian can be approximated as:\n",
    "\n",
    "<br>\n",
    "$$e^{-i T H} \\ \\approx \\ \\displaystyle\\prod_{i \\ = \\ 1}^{T / \\Delta} \\Bigg[ \\displaystyle\\prod_{j \\ = \\ 1}^{Q} e^{-i \\Delta H_{j}(\\boldsymbol\\theta)} \\Bigg] \\ = \\ U_{\\text{RNN}}(\\Delta, \\ \\boldsymbol\\theta)$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, time-evolution is just a case of the QGRNN ansatz. This suggests to us that this ansatz may be particularly useful for learning the dynamics of quantum systems. Let's say that we are given a set $\\{|\\Psi(t)\\rangle\\}_{t \\in T}$ of low-energy states that have been evolved under some unknown Hamiltonian $H$, for a bunch of different times in the set $T$. Our goal is to determine the Hamiltonian of this system, given only this collection of states, and the \"general model\" by which our system evolves (Ising, Heisenberg, etc.). In theory, one doesn't have to know the \"model\" of the system, but our algorithm would require an absurd amount of gates, and a lot of quantum data, so we will stick to the simpler case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Hamiltonian as a Feature Representation\n",
    "\n",
    "You may now be wondering: \"how is this Hamiltonian learning a graph-theoretic problem, fit for a **graph** neural network?\". It is actually very interesting to note that this process of learning the Hamiltonian is actually a form of graph representation learning. Specifically, the $ZZ$ terms of the Hamiltonian will encode exactly where the edges of our graph interactions are. In addition to this, if we consider the state collection of states $\\{ |\\Psi(t)\\rangle \\}$ to be the features associated with the graph, then we are able to determine all of these by simply \"using\" the Hamiltonian (all we have to do is evolve our fixed initial state forward in time by some time $t$). Thus, the Hamiltonian is exactly the node representation that describes the graph along with its features. In this paradigm, however, we are dealing with inherently quantum mechanical data/features. This means that the data can't always necessarily be assigned to one node (in the case of entanglement between nodes), and in many cases, computation required to prepare, store, and process this quantum data scales poorly. Thus, our quantum graph neural network is particularly well-suited to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The QGRNN Applied to an Ising Model\n",
    "\n",
    "In this Notebook we are going to investigate an application of the QGRNN to learning the dynamics of a 4-qubit Ising model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Graph and Preparing the Target Hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin by defining some fixed values that will be used through out the simulation, as well as the device on which we will run our simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the device on which the simulation is run\n",
    "\n",
    "qubit_number = 4\n",
    "qubits = range(qubit_number)\n",
    "vqe_dev = qml.device(\"default.qubit\", wires=qubit_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining the graph on which our Ising model lives. We choose the graph to be the completely connected graph with $3$ nodes, denoted by $K^3$. This is more commonly known as a triangle. We can construct and visualize this graph using `networkx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(0, 1), (0, 3), (1, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates the graph structure of the quantum system\n",
    "\n",
    "ising_graph = nx.Graph()\n",
    "ising_graph.add_nodes_from(range(0, qubit_number))\n",
    "ising_graph.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0)])\n",
    "\n",
    "# Plots the graph\n",
    "\n",
    "nx.draw(ising_graph)\n",
    "\n",
    "print(\"Edges: \"+str(ising_graph.edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the Hamiltonian we will eventiually attempt to learn. There are two reasons for doing this. Firstly, we need to compare the preparred Hamiltonian to the targt Hamiltonian, to make sure that our toy example of the QGRNN actually works, and secondly, we have to prepare the quantum data that will be usd in training. In a real-world application of this algorithm, we would have access to a bunch of quantum data that we could feed into the neural network, but in this scenario, we have to generate the quantum data ourself. The only way to do this is to know the Hamiltonian (and thus, the dynamics) of our system. Our Ising model Hamiltonian will be of the form:\n",
    "\n",
    "<br>\n",
    "$$\\hat{H} \\ = \\ \\displaystyle\\sum_{i, j} \\alpha_{ij} Z_{i} Z_{j} \\ + \\ \\displaystyle\\sum_{i} \\beta_{i} Z_{i} \\ + \\ \\displaystyle\\sum_{i} X_{i}$$\n",
    "<br>\n",
    "\n",
    "Where $\\boldsymbol\\alpha$ and $\\boldsymbol\\beta$ are the matrix and vector of interaction and bias parameters we are trying to learn, respectively. We will initialize $\\boldsymbol\\alpha$ and $\\boldsymbol\\beta$ randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target parameters: [[0.42, -0.11, -0.36, -1.88], [0.04, 0.16, -0.84, 1.51]]\n"
     ]
    }
   ],
   "source": [
    "def create_params(graph):\n",
    "    \n",
    "    # Creates the interaction parameters\n",
    "    interaction = [random.randint(-200, 200)/100 for i in range(0, len(graph.edges))]\n",
    "    \n",
    "    # Creates the bias parameters\n",
    "    bias = [random.randint(-200, 200)/100 for i in range(0, qubit_number)]\n",
    "    \n",
    "    return [interaction, bias]\n",
    "\n",
    "# Creates and prints the parameters for our simulation\n",
    "matrix_params = create_params(ising_graph)\n",
    "print(\"Target parameters: \"+str(matrix_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wtih this knowledge, let's now construct the matrix representation of the Hamiltonian, in the $Z$-basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.06000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00 -1.00000000e-01  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  5.10000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  1.00000000e+00 -1.46000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.50000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00 -5.40000000e-01  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  3.22000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.00000000e+00 -3.34000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.76000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00 -1.24000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  4.40000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.00000000e+00 -2.60000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -5.20000000e-01  1.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00 -2.22044605e-16  0.00000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  4.20000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.00000000e+00 -2.80000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def create_hamiltonian_matrix(n, graph, params):\n",
    "    \n",
    "    # Defines Pauli matrices\n",
    "    pauli_x = np.array([[0, 1], [1, 0]])\n",
    "    pauli_y = np.array([[0, -1j], [1j, 0]])\n",
    "    pauli_z = np.array([[1, 0], [0, -1]])\n",
    "    identity = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "    matrix = np.zeros((2**n, 2**n))\n",
    "    \n",
    "    # Creates the interaction component of the Hamiltonian\n",
    "    for count, i in enumerate(graph.edges):\n",
    "        m = 1\n",
    "        for j in range(0, n):\n",
    "            if (i[0] == j or i[1] == j):\n",
    "                m = np.kron(m, pauli_z)\n",
    "            else:\n",
    "                m = np.kron(m, identity)\n",
    "        matrix = np.add(matrix, params[0][count]*m)\n",
    "    \n",
    "    # Creates the \"bias\" component of the matrix\n",
    "    for i in range(0, n):\n",
    "        m = 1\n",
    "        for j in range(0, n):\n",
    "            if (j == i):\n",
    "                m = np.kron(m, pauli_z)\n",
    "            else:\n",
    "                m = np.kron(m, identity)\n",
    "        matrix = np.add(matrix, params[1][i]*m)\n",
    "    \n",
    "    # Creates the X component of the matrix\n",
    "    for i in range(0, n):\n",
    "        m = 1\n",
    "        for j in range(0, n):\n",
    "            if (j == i):\n",
    "                m = np.kron(m, pauli_x)\n",
    "            else:\n",
    "                m = np.kron(m, identity)\n",
    "        matrix = np.add(matrix, m)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Defines and prints the matrix for our interaction graph and parameters\n",
    "ham_matrix = create_hamiltonian_matrix(qubit_number, ising_graph, matrix_params)\n",
    "print(ham_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Hamiltonian is the main object of interest in the quantum graph neural network, let's visualize it. We can write up a method that allows us to assign colours to the entires of an array, and prints out a nice-looking colour graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creates the grid visualization of the density matrices\n",
    "\n",
    "def create_density_plot(data):\n",
    "\n",
    "    array = np.array(data)\n",
    "    plt.matshow(array)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "create_density_plot([np.diag(ham_matrix), np.diag(ham_matrix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize the **diagonal** elements of the Hamiltonian, as these are the only entries that correspond to trainable parameters (the off-diagonal elements remain fixed, so there is no point in visualizing them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual display of the target Hamiltonian diagonal\n",
    "\n",
    "create_density_plot([np.diag(ham_matrix), np.diag(ham_matrix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Quantum Data with VQE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the target Hamiltonian, we can use it to prepare all of the necessary quantum data that we will use to train our QGRNN. As was discussed above, the data that will be fed into the neural network is a collection of time-evolved, low-energy quantum states corresponding to the target Hamiltonian. Thus, it follows that we must first prepare the low-energy states at $t \\ = \\ 0$, and evolve them forward in time to arbitrary times from $0$ to $T$. In order to prepare low energy states of the Hamiltonian, we will use VQE. Recall that the purpose of VQE is to find the ground energy state of a given Hamiltonian. We don't want to find the exact ground state of the Hamiltonian, as it's time evolution will just equate to the addition of a global phase.\n",
    "\n",
    "<br>\n",
    "$$U(t)|E_k\\rangle \\ = \\ e^{-i H t / \\hbar} |E_k\\rangle \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\Big( -\\frac{i t}{\\hbar} \\Big)^n \\frac{H^n |E_k\\rangle}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{1}{n!} \\Big( \\frac{- i E_k t}{\\hbar} \\Big)^n |E_k\\rangle \\ = \\ e^{i E_k t / \\hbar} |E_k\\rangle$$\n",
    "<br>\n",
    "\n",
    "Thus, this quantum data will be practically useless, as we are essentially just giving ourselves a bunch of copies of the exact same quantum state. Thus, instead of prearing the exact ground state, we wish to prepare a state $|\\phi\\rangle$ that is \"close\" to the ground state, $|\\psi_0\\rangle$ (meaning that $|\\langle \\phi | \\psi_0 \\rangle |^2$ is close to $1$). To do this, we will preform VQE, but we will use an ansataz that we know **won't converge exactly towards the ground state**.\n",
    "\n",
    "For the specific example of the Hamiltonian we are considering in this tutorial, we have $ZZ$ interaction terms between qubits that share an edge on the graph. Thus, we know that it is likely that there will be some kind of entanglement between qubits in the ground state. We will then choose our ansatz such that the qubits remain **unentangled**. By only applying single-qubit gates, we ensure that each of the qubits remains completely uncorrelated from the others, thus making is so that the ground state can never be reached by our optimizer. The initial layer of our variational algorithms will be simply to create an even superposition over all basis states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a method that creates an even superposition of basis states\n",
    "\n",
    "def even_superposition(qubits):\n",
    "    \n",
    "    for i in qubits:\n",
    "        qml.Hadamard(wires=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following layers will be alternating applications of $RZ$ and $RX$ gates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that prepares a low-energy state        \n",
    "        \n",
    "def decoupled_layer(param1, param2, qubits):\n",
    "    \n",
    "    # Applies a layer of RZ and RX gates\n",
    "    for count, i in enumerate(qubits):\n",
    "        \n",
    "        qml.RZ(param1[count], wires=i)\n",
    "        qml.RX(param2[count], wires=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define our VQE ansatz as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that creates the decoupled VQE ansatz\n",
    "\n",
    "def vqe_circuit(parameters, qubits, depth):\n",
    "    \n",
    "    even_superposition(qubits)\n",
    "    \n",
    "    for i in range(0, depth):\n",
    "        decoupled_layer(parameters[0], parameters[1], qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a function and a QNode that allows us to run our VQE circuit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3399575738962546\n",
      " 0: ──H──RZ(1)──RX(1)──RZ(1)──RX(1)──╭┤ ⟨H0⟩ \n",
      " 1: ──H──RZ(1)──RX(1)──RZ(1)──RX(1)──├┤ ⟨H0⟩ \n",
      " 2: ──H──RZ(1)──RX(1)──RZ(1)──RX(1)──├┤ ⟨H0⟩ \n",
      " 3: ──H──RZ(1)──RX(1)──RZ(1)──RX(1)──╰┤ ⟨H0⟩ \n",
      "H0 =\n",
      "[[-1.06000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00 -1.00000000e-01  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  5.10000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  1.00000000e+00 -1.46000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.50000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00 -5.40000000e-01  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  3.22000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.00000000e+00 -3.34000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.76000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00 -1.24000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  4.40000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.00000000e+00 -2.60000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -5.20000000e-01  1.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00 -2.22044605e-16  0.00000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  4.20000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.00000000e+00 -2.80000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defines the depth of our variational circuit\n",
    "vqe_depth = 2\n",
    "\n",
    "# Defines the circuit that we will use to perform VQE on our Hamiltonian\n",
    "def create_circuit(params1, params2):\n",
    "    \n",
    "    vqe_circuit([params1, params2], qubits, vqe_depth)\n",
    "    \n",
    "    return qml.expval(qml.Hermitian(ham_matrix, wires=range(qubit_number)))\n",
    "\n",
    "# Creates the corresponding QNode\n",
    "qnode = qml.QNode(create_circuit, vqe_dev)\n",
    "\n",
    "# Constructs a test case of our circuit\n",
    "resulting_circuit = qnode([1, 1, 1, 1, 1], [1, 1, 1, 1, 1])\n",
    "print(resulting_circuit)\n",
    "print(qnode.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of the code we need to optimize our quantum circuit, thus we create an optimizer and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Step 0: -1.1246631821914508\n",
      "Cost Step 50: -5.097305788738785\n",
      "Cost Step 100: -5.20605098133167\n",
      "Cost Step 150: -5.207131886658464\n",
      "[-4.712710686673246, -4.789301113052475, -4.9705238128623765, 5.191001785669871, 6.306422387087022, -6.663046357510397, -6.932211052853649, -5.466566198442054]\n"
     ]
    }
   ],
   "source": [
    "# Creates the cost function\n",
    "\n",
    "def cost_function(params):\n",
    "\n",
    "    return qnode(params[0:qubit_number], params[qubit_number:2*qubit_number])\n",
    "\n",
    "# Creates the optimizer for VQE\n",
    "\n",
    "optimizer = qml.AdamOptimizer(stepsize=0.8)\n",
    "\n",
    "steps = 200\n",
    "vqe_params = list([random.randint(-100, 100)/10 for i in range(0, 2*qubit_number)])\n",
    "\n",
    "for i in range(0, steps):\n",
    "    vqe_params = optimizer.step(cost_function, vqe_params)\n",
    "    if (i%50 == 0):\n",
    "        print(\"Cost Step \"+str(i)+\": \"+str(cost_function(vqe_params)))\n",
    "\n",
    "print(vqe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check to see if we generated a low-energy state. This will require us to find the ground state energy of the Hamiltonian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.332943284074904\n"
     ]
    }
   ],
   "source": [
    "# Finds the ground state energy of a Hamiltonian\n",
    "\n",
    "def ground_state_energy(matrix):\n",
    "    \n",
    "    # Finds the eigenstates of the matrix\n",
    "    val = np.linalg.eig(matrix)[0]\n",
    "    \n",
    "    # Returns the minimum eigenvalue\n",
    "    return min(val)\n",
    "\n",
    "ground_state = ground_state_energy(ham_matrix)\n",
    "print(ground_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty close to the energy value we found above with our decoupled VQE, so we can conclude that we have generated a low-energy state. With a good initial state, we can now prepare our collection of quantum data by evolving our initial state vector forward in time. We can do this exactly in Pennylane, using a custom unitary gate, which we define to be the time-evolution operator for our Ising model Hamiltonian, which we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an exact time-evolution unitary\n",
    "\n",
    "def state_evolve(hamiltonian, qubits, time):\n",
    "\n",
    "    U = scipy.linalg.expm(complex(0,-1)*hamiltonian*time)\n",
    "    qml.QubitUnitary(U, wires=qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning the Hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we need to prepare our quantum data. Now, let's turn our attention to the actual quantum graph neural network ansatz. Our goal is to learn $\\hat{H}$ by findings the parameters $\\boldsymbol\\alpha$ and $\\boldsymbol\\beta$.\n",
    "\n",
    "The idea behind this protocol is for the QGRNN ansatz, with some inputed parameters, to act like an effective time-evolution operator, under the Hamiltonian with our \"guessed\" parameters. We will then have this effective operator act upon the initial low-energy state prepared by our VQE process, $|\\psi_0\\rangle$. We will also prepare another register of qubits so that it contains one of the samples from our quantum data, $|\\Psi(t)\\rangle$. Finally, we will perform a [SWAP test](https://en.wikipedia.org/wiki/Swap_test) between the two registers, which allows us to compute the inner product between the states contained in the two registers. This will tell us how \"similar\" the two states are. We will finally calculate the average infidelity (one minus the average fidelity) for all of our quantum data, as shown below (we use this as our cost function):\n",
    "\n",
    "<br>\n",
    "$$\\mathcal{L}(\\Delta, \\ \\boldsymbol\\theta) \\ = \\ 1 \\ - \\ \\frac{1}{|T|} \\displaystyle\\sum_{t \\ \\in \\ T} | \\langle \\Psi(t) | \\ U_{\\text{RNN}}(\\Delta, \\ \\boldsymbol\\theta) \\ |\\psi_0\\rangle |^2$$\n",
    "<br>\n",
    "\n",
    "As the parameters of our learned Hamiltonian approach those of the true Hamiltonian ($\\boldsymbol\\alpha$ and $\\boldsymbol\\beta$), our QGRNN ansatz becomes a good approximation of the time-evolution operator under the true Hamiltonian. This means that the states in the two registers are essentially the same, and infidelity approaches $0$. Thus, as we minimize the cost function, we reconstruct the true Hamiltonian.\n",
    "\n",
    "Here is a diagram explaining the general procedure of the algorithm:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/qgrnn.png\" style=\"width:800px;\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Our Ising Hamiltonian involves $ZZ$ gates, thus we need to write a method that allows us to construct $RZZ$ gates out of the standard gate set in Pennylane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the RZZ gate, in terms of gates in the standard basis set\n",
    "\n",
    "def RZZ(param, qubit1, qubit2):\n",
    "    \n",
    "    qml.CNOT(wires=[qubit1, qubit2])\n",
    "    qml.RZ(param, wires=qubit2)\n",
    "    qml.CNOT(wires=[qubit1, qubit2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the implemented gate is equivalent to the $RZZ$ gate. With this, we can now write a function that implements one step of the Trotterized time evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that prepares a time-evolution layer\n",
    "\n",
    "def qgrnn_layer(param1, param2, qubits, graph, trotter):\n",
    "    \n",
    "    # Applies a layer of coupling gates (based on the graph)\n",
    "    for count, i in enumerate(graph.edges):\n",
    "        RZZ(2*param1[count]*trotter, i[0], i[1])\n",
    "    \n",
    "    # Applies a layer of RZ gates\n",
    "    for count, i in enumerate(qubits):\n",
    "        qml.RZ(2*param2[count]*trotter, wires=i)\n",
    "    \n",
    "    # Applies a layer of RX gates\n",
    "    for i in qubits:\n",
    "        qml.RX(2*trotter, wires=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will need to make use of the $SWAP$ test, to calculate the fidelity between the prepared quantum state and our quantum data. As it turns out, the inner product of the two states ends up simply being $\\langle Z \\rangle$, with respect to the ancilla qubit. When we perform a $SWAP$ test, we get:\n",
    "\n",
    "<br>\n",
    "$$\\text{P}(\\text{Ancilla} \\ = \\ 0) \\ = \\ \\frac{1}{2} \\ + \\ \\frac{1}{2} | \\langle \\psi | \\phi \\rangle |^2$$\n",
    "<br>\n",
    "\n",
    "Where $|\\psi\\rangle$ and $|\\phi\\rangle$ are the states contained in the two registers. We also have:\n",
    "\n",
    "<br>\n",
    "$$\\langle Z \\rangle \\ = \\ (1) \\text{P}(\\text{Ancilla} \\ = \\ 0) \\ + \\ (-1) \\text{P}(\\text{Ancilla} \\ = \\ 1) \\ = \\ \\text{P}(\\text{Ancilla} \\ = \\ 0) \\ - \\ \\big(1 \\ - \\ \\text{P}(\\text{Ancilla} \\ = \\ 0)\\big) \\ = \\ 2\\text{P}(\\text{Ancilla} \\ = \\ 0) \\ - \\ 1$$\n",
    "<br>\n",
    "\n",
    "So we have:\n",
    "\n",
    "<br>\n",
    "$$\\text{P}(\\text{Ancilla} \\ = \\ 0) \\ = \\ \\frac{1}{2} \\ + \\ \\frac{1}{2} \\langle Z \\rangle$$\n",
    "<br>\n",
    "\n",
    "Thus, from the first and the third equation, we get $\\langle Z \\rangle \\ = \\ |\\langle \\psi | \\phi \\rangle|^2$. We can write a function that performs this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the SWAP test between two qubit registers\n",
    "\n",
    "def swap_test(control, register1, register2):\n",
    "    \n",
    "    qml.Hadamard(wires=control)\n",
    "    for i in range(0, len(register1)):\n",
    "        qml.CSWAP(wires=[int(control), register1[i], register2[i]])\n",
    "    qml.Hadamard(wires=control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can build our quantum circuit that corresponds to one execution of our QGRNN, for a given time step. First, let's define a new quantum device with $7$ qubits rather than $3$, as we will need registers for our neural network, and data-preparation, plus an extra qubit for the $SWAP$ test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the new\n",
    "qgrnn_dev = qml.device(\"default.qubit\", wires=2*qubit_number+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a few more fixed variables needed for our simulation. Here, we can define the two registers of qubits, the index of the ancilla qubit (for the $SWAP$ test), the size of the Trotterization steps, and a new graph of interations. It is important to note that we do not know the interaction structure (which qubits are \"connected\" by $ZZ$ interactions) of our model before we begin the simulation. Thus, we initialize our model in a more general structure than the square-shaped graph that we utilized at the beginning of this tutorial. We thus choose the most general graph of interactions for $4$ qubits, the complete graph $K^4$. We then hope that as our algorithm converges, it learn the structure of the graph by setting the parameters of edge $(0, \\ 2)$ and $(1, \\ 3)$ to $0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(4, 5), (4, 6), (4, 7), (5, 6), (5, 7), (6, 7)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defines some fixed values\n",
    "\n",
    "reg1 = list(range(qubit_number))\n",
    "reg2 = list(range(qubit_number, 2*qubit_number))\n",
    "\n",
    "control = 2*qubit_number\n",
    "trotter_step = 0.01\n",
    "\n",
    "# Defines the interaction graph for the new qubit system\n",
    "\n",
    "new_ising_graph = nx.Graph()\n",
    "new_ising_graph.add_nodes_from(range(qubit_number, 2*qubit_number))\n",
    "new_ising_graph.add_edges_from([(4, 5), (5, 6), (6, 7), (4, 6), (7, 4), (5, 7)])\n",
    "\n",
    "print(\"Edges: \"+str(new_ising_graph.edges))\n",
    "nx.draw(new_ising_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can implement the QGRNN ansatz, as we outlined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the quantum graph neural network for a given time step\n",
    "\n",
    "def qgrnn(params1, params2, time):\n",
    "    \n",
    "    # Prepares the low energy state in the two qubit registers\n",
    "    vqe_circuit([vqe_params[0:qubit_number], vqe_params[qubit_number:2*qubit_number]], reg1, vqe_depth)\n",
    "    vqe_circuit([vqe_params[0:qubit_number], vqe_params[qubit_number:2*qubit_number]], reg2, vqe_depth)\n",
    "    \n",
    "    # Evolves the first qubit register with the time-evolution circuit\n",
    "    state_evolve(ham_matrix, reg1, time.val)\n",
    "    \n",
    "    # Applies the time-evolution layers to the second qubit register\n",
    "    depth = time.val/trotter_step\n",
    "    for i in range(0, int(depth)):\n",
    "        #qgrnn_layer(matrix_params[0], matrix_params[1], reg1, ising_graph, trotter_step)\n",
    "        qgrnn_layer(params1, params2, reg2, new_ising_graph, trotter_step)\n",
    "    \n",
    "    # Applies the SWAP test between the registers\n",
    "    swap_test(control, reg1, reg2)\n",
    "    \n",
    "    # Returns the results of the SWAP test\n",
    "    return qml.expval(qml.PauliZ(control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are new tasked with constructing the cost function of our model. To evaluate the cost function, we simply have to choose a bunch of time steps at which we will execute of QGRNN. We begin by defining a few more values, along with our new QNode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 15 # The number of different times that will be used\n",
    "max_time = 0.1  # The maximum value of time that can be utilized\n",
    "\n",
    "# Defines the new QNode\n",
    "\n",
    "qnode = qml.QNode(qgrnn, qgrnn_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we define the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the cost function\n",
    "\n",
    "def cost_function(params):\n",
    "     \n",
    "    global iterations\n",
    "    \n",
    "    # Separates the parameter list\n",
    "    weight_params = params[0:6]\n",
    "    bias_params = params[6:10]\n",
    "    \n",
    "    # Samples times at which the QGRNN will be run\n",
    "    times_sampled = [np.random.uniform() * max_time for i in range(0, batch)]\n",
    "     \n",
    "    # Cycles through each of the sampled times and calculates the cost\n",
    "    total_cost = 0\n",
    "    for i in times_sampled:\n",
    "        result = qnode(weight_params, bias_params, i)\n",
    "        total_cost += 1 - result\n",
    "    \n",
    "    # Prints the value of the cost function\n",
    "    print(\"Cost at Step \"+str(iterations)+\": \"+str((1 - total_cost / batch)._value)+\" - \"+str(params._value))\n",
    "    iterations += 1\n",
    "    \n",
    "    return total_cost / batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we execute the optimization method using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at Step 0: 0.9629573496827061 - [0.9, 1.8, 1.0, -1.7, -0.3, -0.4, 0.4, 0.8, -1.3, -2.0]\n",
      "Cost at Step 1: 0.993835052565473 - [0.4000009965411841, 1.3000005543132656, 0.5000006158669671, -1.2000009386524224, 0.19999899649018815, -0.8999986212072486, 0.8999993860397126, 0.300000848999061, -0.8000022613142977, -1.5000028656507927]\n",
      "Cost at Step 2: 0.9939453995294547 - [0.06235334253042357, 0.9592829527840567, 0.16074462644428672, -0.8598518592408866, 0.5380291797795091, -1.2423838861746246, 1.2391396677111335, -0.03911520054299733, -0.45967454451496215, -1.154717487315844]\n",
      "Cost at Step 3: 0.990260242842246 - [-0.17922709407500012, 0.6916871082203568, -0.09337678207983391, -0.5977156950240843, 0.7800445031434454, -1.5245563987598685, 1.492109700498792, -0.2913697923618386, -0.19502261654612207, -0.8476531622825769]\n",
      "Cost at Step 4: 0.9832212238814403 - [-0.3467453786124589, 0.48101094701888925, -0.2813222093165536, -0.3964125837251262, 0.9476732262118921, -1.7595474163732008, 1.6780359190438636, -0.47609168722577155, 0.009969861526455348, -0.5703170339261361]\n",
      "Cost at Step 5: 0.9745586263619568 - [-0.47243696669226665, 0.3094839289082249, -0.428412770665379, -0.2348603921972204, 1.0731477266261662, -1.956897898609622, 1.8229189572727886, -0.6197808253563332, 0.17467531101752526, -0.3265459910466212]\n",
      "Cost at Step 6: 0.9533653200096667 - [-0.5460130553916464, 0.17839013999967368, -0.5282659412444057, -0.11614089275065367, 1.1461315163785215, -2.1198371951836736, 1.9198651063881869, -0.7155740956020028, 0.2947905731793927, -0.1026496474939051]\n",
      "Cost at Step 7: 0.9534690151276295 - [-0.5439367382614302, 0.10096721604183714, -0.5603057303878076, -0.055629178119489295, 1.141206535608722, -2.2421659068715227, 1.9474842633733638, -0.742430037536059, 0.3524481230898538, 0.11339475930654389]\n",
      "Cost at Step 8: 0.9688635305397125 - [-0.5010986139955098, 0.056809919426182964, -0.5537494464592564, -0.029319184892563202, 1.0950737657268026, -2.3346115548817004, 1.9359183281198031, -0.730848607989788, 0.3708776904080009, 0.3149214254838576]\n",
      "Cost at Step 9: 0.9686504774670849 - [-0.46273345026893775, 0.017656885762448685, -0.5477162916773586, -0.006062162859731664, 1.0537897219184749, -2.4167686581345005, 1.9254211873188043, -0.720344738957715, 0.38707227077066936, 0.4942877974634541]\n",
      "Cost at Step 10: 0.9634011412424309 - [-0.3909092788331354, 0.005968902315207631, -0.5095147982532069, -0.00976875410258527, 0.979407922166225, -2.471285348304009, 1.8822053456380006, -0.6787114422746623, 0.3665967611098212, 0.6614787400292254]\n",
      "Cost at Step 11: 0.9477776266363619 - [-0.3164116118659532, 0.0018476756613125343, -0.4662570688584155, -0.019556643956631703, 0.9025301161219901, -2.5145968582973466, 1.834146628740414, -0.6326569961815217, 0.3381931852025311, 0.8131232617203397]\n",
      "Cost at Step 12: 0.9794939505485359 - [-0.19150963316402125, 0.0379229765981912, -0.3735486088687027, -0.06735596703935169, 0.7754035820905018, -2.515247440811046, 1.73548122157124, -0.5398757164205743, 0.2496626937291807, 0.9565830313525199]\n",
      "Cost at Step 13: 0.9756771329042742 - [-0.0728095218272865, 0.07441200213864055, -0.2843312256083176, -0.11419088872115003, 0.6547241197365735, -2.5116207433250324, 1.640690390151278, -0.45095911076522543, 0.16274493864702672, 1.0867374781496928]\n",
      "Cost at Step 14: 0.9826880217028455 - [0.07216536516352812, 0.13398872347891472, -0.1671271916849952, -0.1802137016058965, 0.5082603270710364, -2.476604650432178, 1.5171479935972791, -0.3372426772031795, 0.035228952318979045, 1.2066788005886846]\n",
      "Cost at Step 15: 0.9946526037739435 - [0.23043502133586066, 0.20711961603811965, -0.0338944617804007, -0.25575128359219607, 0.34901108624158506, -2.418345374918015, 1.3770564870734736, -0.21045199244522908, -0.12009138930891824, 1.3159305417556488]\n",
      "Cost at Step 16: 0.997569354840856 - [0.37802132796139326, 0.276070857712339, 0.09096377493065395, -0.3262108656358913, 0.2008413960111503, -2.361056120639099, 1.2457508333110765, -0.09213888793779222, -0.26792627072927844, 1.4150291283190983]\n",
      "Cost at Step 17: 0.9980384153284942 - [0.5145272996333069, 0.3403689455325245, 0.20719992894596753, -0.3910438337134173, 0.06397426740720327, -2.3050042238270807, 1.1234397252449575, 0.0173849825899981, -0.40829116888570205, 1.504784817124708]\n",
      "Cost at Step 18: 0.9987712171675871 - [0.6390371075193416, 0.3990287549819328, 0.3133057656304229, -0.4500429617474832, -0.06075916904144592, -2.2533413632810197, 1.0117620668368268, 0.11722574894946823, -0.5370452354463857, 1.586375782192836]\n",
      "Cost at Step 19: 0.9986567910768329 - [0.7505477184121127, 0.45177831381537337, 0.411965265479003, -0.49888898704697165, -0.17226486641013727, -2.194128567149867, 0.9072143811609495, 0.2066833280230877, -0.6712793879784533, 1.6575787621379077]\n",
      "Cost at Step 20: 0.9980969547302272 - [0.8521943970183757, 0.4998617870442401, 0.5018976084819212, -0.5434142227528869, -0.27390706612586135, -2.140153577868684, 0.8119147055756795, 0.2882277274862613, -0.793639542600643, 1.7224833143211378]\n",
      "Cost at Step 21: 0.9956125143059275 - [0.9434916834423408, 0.5427844376727115, 0.583122741258946, -0.5825955339572938, -0.36512397774628835, -2.0899663690721617, 0.7256888176419554, 0.361353635878777, -0.9064832584744114, 1.7811029514985683]\n",
      "Cost at Step 22: 0.9936753374956658 - [1.0080264829291445, 0.5702729964867616, 0.6458984303454768, -0.6019142747645959, -0.43023756592051915, -2.0377125374504788, 0.6574624833075424, 0.41275756769980027, -1.016448284258974, 1.8289059130163983]\n",
      "Cost at Step 23: 0.9966108299245462 - [1.03813921632417, 0.5770956677515832, 0.6844044137706444, -0.5961142325622237, -0.4621447862564283, -1.9852816124458212, 0.6128633420797895, 0.4362778000251043, -1.1201724887998408, 1.8646942434052578]\n",
      "Cost at Step 24: 0.9916777689685186 - [1.0620074894047624, 0.580943004286432, 0.7170436971056152, -0.5880481719475972, -0.4878126560818979, -1.937344854087944, 0.5745248932487146, 0.45479562406812585, -1.2145338182483087, 1.8963997181084007]\n",
      "Cost at Step 25: 0.9925933180849474 - [1.0521529140798134, 0.5640590927166579, 0.724820710883573, -0.5584165499770775, -0.4823821163832035, -1.8961135431773863, 0.5605066584169098, 0.4470388268449967, -1.2927554439958477, 1.9175834802317744]\n",
      "Cost at Step 26: 0.9952295364657003 - [1.0137969881821118, 0.5295875249366567, 0.71115256439138, -0.5123057296350115, -0.45139863160629384, -1.8638542241628486, 0.5675044437230424, 0.4176927786203958, -1.3520585617338141, 1.929739091871228]\n",
      "Cost at Step 27: 0.994711254099609 - [0.9628995708039876, 0.4878098207333155, 0.6874115191351609, -0.46101495228852163, -0.40956174116925703, -1.839017388302304, 0.5845719839964029, 0.37935228144400324, -1.3969356563113269, 1.9368266871381967]\n",
      "Cost at Step 28: 0.9962472826843394 - [0.894991201137966, 0.43566423176963837, 0.6504009758838539, -0.40355882529928744, -0.35377167370585544, -1.8252796671999785, 0.6145843594481573, 0.3294456531925616, -1.4210749410145456, 1.937515341991549]\n",
      "Cost at Step 29: 0.9966306086005318 - [0.8315367933215478, 0.38713093729650383, 0.6156064769491388, -0.35054324035171125, -0.3017379280752391, -1.8135082913913452, 0.6429128847371045, 0.2829650145256141, -1.4416859412442244, 1.937718819626967]\n",
      "Cost at Step 30: 0.998405722884338 - [0.7502887495533967, 0.32759024745876825, 0.5673067914883709, -0.29664446126326216, -0.23763900481773595, -1.8205015807512392, 0.6838207205662796, 0.22691417653018148, -1.4291642160042013, 1.9300230971675514]\n",
      "Cost at Step 31: 0.9991345824292288 - [0.6665366324134295, 0.2669324761643835, 0.516491064071692, -0.2474968627865552, -0.1731029982718137, -1.8369973840178766, 0.7270027360815479, 0.17104558567547462, -1.4002305567833837, 1.9188283387643743]\n",
      "Cost at Step 32: 0.9988808411842532 - [0.5840138527902414, 0.2077376333275, 0.466196689344556, -0.20532772691706624, -0.11151031794476635, -1.8613081170479058, 0.7694413384288435, 0.11829709163674902, -1.3577185467608444, 1.9048141635685607]\n",
      "Cost at Step 33: 0.9994907238456036 - [0.5056329626140794, 0.15182511684799016, 0.41868564625346233, -0.17259967475959515, -0.055513135213528436, -1.8931651984168807, 0.808745590509851, 0.07106106517149252, -1.3021602817363607, 1.8879851855267733]\n",
      "Cost at Step 34: 0.9993973871824559 - [0.4340672826812839, 0.10077418766375038, 0.37530568326378805, -0.14271732334368217, -0.004384901698827125, -1.9222523097130972, 0.8446323420896431, 0.027932206247757177, -1.2514328027465638, 1.872619485730318]\n",
      "Cost at Step 35: 0.9987993547032787 - [0.3687308637497799, 0.054166862590247465, 0.3357016237805065, -0.11543600872788926, 0.042292998754178183, -1.9488076038180728, 0.8773954185506931, -0.011442605815594621, -1.2051207747250945, 1.8585912563242895]\n",
      "Cost at Step 36: 0.9975610797740034 - [0.3164678863127348, 0.016776975767551522, 0.30612735128928503, -0.10407660930727566, 0.07552991899069547, -1.9801244459695124, 0.8993949662886781, -0.038219717048524116, -1.1497964291789713, 1.8415574915260233]\n",
      "Cost at Step 37: 0.9977535064953054 - [0.27703744894599747, -0.011489038613626198, 0.28643404055675287, -0.10605384216652419, 0.09630241628823447, -2.013211636855825, 0.911027117395645, -0.05345420708619808, -1.090375651833574, 1.8224927224142833]\n",
      "Cost at Step 38: 0.9980864454147118 - [0.24804843512762104, -0.032405792311071636, 0.2744594545316421, -0.11670710535778868, 0.10789430238408222, -2.045417873917449, 0.9148235928903284, -0.06031888166677046, -1.0316266155954699, 1.8026332651032413]\n",
      "Cost at Step 39: 0.9982001073410791 - [0.22407551811825374, -0.04989041553208917, 0.2654717289512164, -0.1290587696719593, 0.11621681845396933, -2.075089834337312, 0.9160915284794491, -0.06443018294115671, -0.9772183857900925, 1.7837510622144224]\n",
      "Cost at Step 40: 0.997692022211491 - [0.20563014677381128, -0.06367601516720929, 0.25987283743651124, -0.14354404676588847, 0.12092784485033957, -2.1021004580712765, 0.9143166191536337, -0.06543465548553834, -0.9272308243253804, 1.7655571526303386]\n",
      "Cost at Step 41: 0.9983795990247623 - [0.20375936711051454, -0.06667152503211256, 0.26621171639735736, -0.1695242795856538, 0.1131039110996482, -2.1246575219414243, 0.8998681918949741, -0.0548188562453277, -0.8828056312498417, 1.744909938297732]\n",
      "Cost at Step 42: 0.998283466293241 - [0.21524297662065828, -0.060712880269980736, 0.2823234661605837, -0.2035025184177186, 0.09556661724365645, -2.1415826519940717, 0.8750911924464186, -0.03521060386942594, -0.8456758721928891, 1.7223239534245294]\n",
      "Cost at Step 43: 0.9988321721503002 - [0.2425436726337808, -0.044340104380232004, 0.30992945071146294, -0.24600806700974315, 0.06732180709383101, -2.1502523475796678, 0.8378766536641384, -0.005556022188778154, -0.8192126053076516, 1.6966421111828187]\n",
      "Cost at Step 44: 0.9988533126826377 - [0.27512607533742184, -0.02445602163777395, 0.34089647251437055, -0.28917311486579234, 0.0365059616928262, -2.1537531625054758, 0.7972449865888434, 0.02642434428240574, -0.8001928345377148, 1.6705467062605828]\n",
      "Cost at Step 45: 0.9995774514928534 - [0.30751715378760164, -0.004684785887995507, 0.371016696524736, -0.3296554055261901, 0.006962362875775483, -2.154877605151386, 0.7579346632030842, 0.05704078297020571, -0.7853652647837928, 1.645648520590351]\n",
      "Cost at Step 46: 0.9997131920551371 - [0.337045044905572, 0.013338785007055935, 0.3984744639357831, -0.3665592916418465, -0.019969749629356662, -2.1559026534488246, 0.7220991489625718, 0.08495090242899318, -0.7718483681561891, 1.6229511936546395]\n",
      "Cost at Step 47: 0.9994317448890091 - [0.36401356301820953, 0.029783561712802206, 0.4235187467467507, -0.40020172323944114, -0.04452226985694899, -2.156797437251654, 0.6894107447533215, 0.1103972756788394, -0.7595903699176039, 1.6022419778341712]\n",
      "Cost at Step 48: 0.9996409723186072 - [0.39628916541263326, 0.048057988413611766, 0.449494144335568, -0.4256425442841383, -0.06481999024398746, -2.1400877865060366, 0.6540231851986096, 0.13263596401180075, -0.7714269292913082, 1.5765016109090415]\n",
      "Cost at Step 49: 0.9996569674725545 - [0.42569921631381075, 0.06470994164645215, 0.4731633468112473, -0.44882463186862015, -0.08331560133686011, -2.1248616825907285, 0.6217774684809446, 0.15290021651283267, -0.782212593473915, 1.5530465728498688]\n",
      "Cost at Step 50: 0.9996116271158044 - [0.4528079789856562, 0.07870035689944187, 0.4926625892428065, -0.462968371426521, -0.09503806073237828, -2.100585960812219, 0.5928969822420853, 0.1672512378681953, -0.8058539995284749, 1.5278885016098092]\n",
      "Cost at Step 51: 0.9995640776367039 - [0.47674095660214494, 0.09028875008511451, 0.508714001451181, -0.472316365091558, -0.10288474235173484, -2.074367147061164, 0.5676010013875677, 0.1779745674403947, -0.8328612593743575, 1.5034904129724431]\n",
      "Cost at Step 52: 0.9996654202006476 - [0.49505508522129144, 0.09666372612270865, 0.5174413369533298, -0.4717150280717421, -0.10214508741411535, -2.0421900851845916, 0.5487445436285496, 0.18106111643321987, -0.8687427036418124, 1.478216570395617]\n",
      "Cost at Step 53: 0.9996677955099902 - [0.5115649803283673, 0.10213131549863252, 0.5249128079222314, -0.47065035856205056, -0.10093971363194738, -2.0125464202636074, 0.5319286481348575, 0.1834161483457998, -0.9019719747349383, 1.4550400524338354]\n",
      "Cost at Step 54: 0.9996279904381047 - [0.5265988413923575, 0.10711008603837784, 0.5317163070473804, -0.4696808736939421, -0.09984210387421164, -1.9855529878891172, 0.5166161439754988, 0.1855606333582167, -0.9322304490836633, 1.4339355850954723]\n",
      "Cost at Step 55: 0.9996762231654394 - [0.5377267520593036, 0.10951374380958558, 0.5350072593984807, -0.4661993370578233, -0.0962822921529851, -1.9603149294635946, 0.5050430784226912, 0.18526601224520675, -0.9606300807775755, 1.4144109366874322]\n",
      "Cost at Step 56: 0.9993815426720362 - [0.5478571936593123, 0.11170194566601571, 0.5380032216888981, -0.4630298738396487, -0.09304157016814163, -1.9373391271745453, 0.49450738410564554, 0.18499779999296084, -0.9864840631392352, 1.3966364135758216]\n",
      "Cost at Step 57: 0.9993013628247545 - [0.5516182763071711, 0.10945239526619918, 0.5349747683488977, -0.45628110412774603, -0.08590836727974639, -1.91734141819476, 0.48975831687699994, 0.18101116905482137, -1.008687847174725, 1.3806018844297365]\n",
      "Cost at Step 58: 0.9995445288142056 - [0.5507415634280128, 0.10409564631127455, 0.527700465241499, -0.44755842464517503, -0.07644140055869915, -1.9005831821253403, 0.48924018363361893, 0.17469611731460646, -1.0269070176994162, 1.3663531402462985]\n",
      "Cost at Step 59: 0.9995206648226757 - [0.5499437288309718, 0.09922084663208053, 0.5210806346800404, -0.4396205288617018, -0.06782618141736073, -1.8853326927641594, 0.48876866708707944, 0.16894923382987148, -1.0434870003852823, 1.3533863628153535]\n",
      "Cost at Step 60: 0.9996324699296157 - [0.5402958607883875, 0.08756821683287913, 0.5049931720536941, -0.4288482946658556, -0.05473759547818746, -1.8779721755925805, 0.49670802112344775, 0.1588609722636811, -1.050170642186348, 1.3433707805623398]\n",
      "Cost at Step 61: 0.9996739909027247 - [0.5315180526459568, 0.0769664394537175, 0.4903565020282147, -0.41904751748136454, -0.042829358787409934, -1.871275441468489, 0.5039313917663477, 0.14968248596725778, -1.0562515425758245, 1.3342584194969402]\n",
      "Cost at Step 62: 0.9996989238855005 - [0.5215675689294792, 0.06581682563041683, 0.4748402327914951, -0.40981040664166063, -0.031216121865730584, -1.8673437015145586, 0.5122409057461623, 0.14057174735567815, -1.058962139930698, 1.3266508945482625]\n",
      "Cost at Step 63: 0.9997497219935444 - [0.5125164447932259, 0.055674952923645396, 0.46072637827413804, -0.401408178246353, -0.02065252998609408, -1.8637673260159187, 0.519799376770355, 0.1322844692102199, -1.0614277439623936, 1.3197309645434776]\n",
      "Cost at Step 64: 0.9998287373716599 - [0.5011222930779845, 0.0441018405808953, 0.444124604053396, -0.394397487790047, -0.010618143446319386, -1.8660732328736962, 0.529364768741403, 0.12418755410456313, -1.056505753873966, 1.3153357193123272]\n",
      "Cost at Step 65: 0.9997874265695634 - [0.4886116399326277, 0.03200899144605786, 0.4262773479312844, -0.38908279215582514, -0.001652897524376449, -1.8732006956770682, 0.5398322361485917, 0.11679853121302704, -1.0456118534036891, 1.3131415970395457]\n",
      "Cost at Step 66: 0.9998176191947099 - [0.4739060273803589, 0.01905402353457615, 0.4059206747366406, -0.3869390201805925, 0.0051961716423070585, -1.889222031817519, 0.5514680619011142, 0.11089595639563803, -1.0234561256177839, 1.3148728653208668]\n",
      "Cost at Step 67: 0.9998591579539093 - [0.45964668677182896, 0.006980117729471439, 0.3861516501570487, -0.3864615147005344, 0.010391306020585413, -1.9079311926161489, 0.5622630590399418, 0.10628202331307804, -0.9980605455734879, 1.3182832686737145]\n",
      "Cost at Step 68: 0.9998503712916066 - [0.4469807326185902, -0.0032676471705759616, 0.367418908289091, -0.38877849536093545, 0.01278340165115649, -1.930894530230093, 0.5709191168218773, 0.1038879816142775, -0.9678647528962001, 1.324465681878284]\n",
      "Cost at Step 69: 0.9998808847390848 - [0.4368964857474171, -0.010981197465326067, 0.3509321513220861, -0.392834483701553, 0.012834160715314197, -1.9550150404313307, 0.5765809834557007, 0.10340013118158191, -0.9369015987290352, 1.332398662549331]\n",
      "Cost at Step 70: 0.9998883626069938 - [0.43053993097034887, -0.015228764322512627, 0.3376976746219295, -0.3980422440082714, 0.010556281133120779, -1.9783701514170673, 0.5780345917793697, 0.10483655428315737, -0.9079491403387602, 1.3418130051696247]\n",
      "Cost at Step 71: 0.9998935928775396 - [0.42755529214773164, -0.016557921354243516, 0.3275550222449125, -0.40325855038075664, 0.007018118179702829, -1.9989789035604062, 0.5759941166434074, 0.1073431670168927, -0.8834419301552082, 1.3516433159821788]\n",
      "Cost at Step 72: 0.9998369347764084 - [0.4278443252416622, -0.015192627520963092, 0.3203871104128922, -0.40781747212552627, 0.0027859929259469295, -2.0157170068793664, 0.5707099190789832, 0.11046281171787659, -0.8648654333659203, 1.3614039579471526]\n",
      "Cost at Step 73: 0.9998240770030826 - [0.4322873972551708, -0.009835858597841224, 0.3172455439039527, -0.41040566950544966, -0.002060024874076566, -2.0255961950036654, 0.5603629712919916, 0.11413940920735824, -0.8564582661422551, 1.3710729122305294]\n",
      "Cost at Step 74: 0.9998634501712171 - [0.4403215386256134, -0.0009171121139562709, 0.31760624867825504, -0.41019600318127025, -0.00688950827281095, -2.027449072680515, 0.5453663916151771, 0.11785645369691274, -0.8597648483766347, 1.3802705424018475]\n",
      "Cost at Step 75: 0.999849521765447 - [0.45063100955584406, 0.009820757924040031, 0.3197165693292679, -0.40770812647731147, -0.011158955954918738, -2.023147399579187, 0.5279157889937709, 0.12115800278155249, -0.872171763007353, 1.3886841891110506]\n",
      "Cost at Step 76: 0.9999239079363895 - [0.4617321381856304, 0.021530571377988306, 0.3223604822441986, -0.4024224975630321, -0.014460686962097654, -2.0123947828350914, 0.508829457431557, 0.12371227634151138, -0.8938837887129577, 1.3962232591265265]\n",
      "Cost at Step 77: 0.9999148764658837 - [0.47207773143045517, 0.03229665845719191, 0.3244672017187635, -0.39703477108311847, -0.01724765315595994, -2.0013176481779142, 0.49107652901793847, 0.12586814187545797, -0.915781849986763, 1.4030263039524897]\n",
      "Cost at Step 78: 0.9999095267402541 - [0.48026121384616655, 0.041341546911126945, 0.32433258231296047, -0.3907884472481775, -0.01912702787507666, -1.9889472513817639, 0.47510598936366855, 0.12732225263872438, -0.9393382711818227, 1.4090962775896678]\n",
      "Cost at Step 79: 0.9999342552708452 - [0.485849870457162, 0.0479065785286182, 0.3211098365147844, -0.3845995722581132, -0.020224685749245255, -1.9772651514715742, 0.46182277425827967, 0.12818592810374999, -0.961887913979371, 1.414544259466392]\n",
      "Cost at Step 80: 0.9999064633915984 - [0.4901767396669075, 0.053182382383633035, 0.3170246306857132, -0.37905184839541, -0.021075937540635675, -1.9670379622815866, 0.4503499245634191, 0.12886780350343535, -0.9819714818609104, 1.419489274013145]\n",
      "Cost at Step 81: 0.9998808234219855 - [0.49247054189108935, 0.05674523550576711, 0.3113899039917618, -0.3742929613097702, -0.021693232034232436, -1.9588618611890571, 0.4410507409341561, 0.1293905960551263, -0.9986356338669434, 1.4240101608823834]\n",
      "Cost at Step 82: 0.9999179518898964 - [0.49177551805408265, 0.05799065129640992, 0.3032671353186474, -0.3706700064899044, -0.02210540901065513, -1.953795726912856, 0.43446060846189044, 0.1297990846062363, -1.01025567560763, 1.428182923106881]\n",
      "Cost at Step 83: 0.9999048889620379 - [0.4890714401684384, 0.057421213042723, 0.29334073026702584, -0.36831217840177605, -0.022405852312177593, -1.9517772776590145, 0.4300513628483094, 0.13016507919809642, -1.017111050066885, 1.43201589254763]\n",
      "Cost at Step 84: 0.9999263866744038 - [0.48361530588657786, 0.05418274976246467, 0.2802439801604593, -0.36809110318702526, -0.022603711215797913, -1.9548741581917444, 0.42848923469229017, 0.1305445958859967, -1.0163455818054095, 1.4355220115528662]\n",
      "Cost at Step 85: 0.9999588455047359 - [0.4764043409543089, 0.049587604693918666, 0.2658045676700554, -0.36907263080313596, -0.022707575452656302, -1.9609936892159727, 0.4284259062870925, 0.13091561822099199, -1.0106289438455713, 1.4386408483300346]\n",
      "Cost at Step 86: 0.9999358337183974 - [0.4698577435059182, 0.04541582388378799, 0.25269550081483017, -0.3699637273822987, -0.022801870370779333, -1.9665494097434915, 0.4283684125145315, 0.13125245717354303, -1.0054389967030755, 1.4414723373837097]\n",
      "Cost at Step 87: 0.9999598800884598 - [0.46283478481776685, 0.040575359567167193, 0.23883611828427936, -0.37218563142202504, -0.022639590331928167, -1.9752386783122122, 0.42863017481818083, 0.13150123065388875, -0.9952180659967836, 1.4436136875099064]\n",
      "Cost at Step 88: 0.9999528060830026 - [0.4564554453785578, 0.03610594494666893, 0.22603215904083648, -0.3744261584602956, -0.022421500936178538, -1.983675924043918, 0.4287642687146229, 0.1316997985240077, -0.9851102025307039, 1.445432255810267]\n",
      "Cost at Step 89: 0.9999320142569855 - [0.4506649106899617, 0.032049051474851764, 0.2144099885648873, -0.37645988775973227, -0.02222354090863785, -1.9913344228886378, 0.4288859859190227, 0.1318800388376651, -0.9759352819020899, 1.4470829725662977]\n",
      "Cost at Step 90: 0.9999380109716335 - [0.4455641243317332, 0.02862582832739814, 0.20400611458504153, -0.37830580234547706, -0.021868946303651448, -1.9984572645696648, 0.42834172594215736, 0.1319554316540104, -0.9670624223297773, 1.4482743411551995]\n",
      "Cost at Step 91: 0.9999559821828908 - [0.4409346662828847, 0.02551892148617808, 0.19456359043816707, -0.3799811487590432, -0.02154711732816658, -2.004921933777386, 0.4278477572547141, 0.13202385794289972, -0.9590094423454343, 1.4493556235978033]\n",
      "Cost at Step 92: 0.999931026700782 - [0.43885255591071787, 0.023782866863221192, 0.1869731501031544, -0.38157024420513225, -0.02082686135757032, -2.010549896619224, 0.42541607423478317, 0.13185014622841373, -0.9520496580557197, 1.4495399584952968]\n",
      "Cost at Step 93: 0.9999368883460209 - [0.44042228990747717, 0.024155807048960178, 0.18196359945823817, -0.3828271733704113, -0.019477773844554475, -2.0145611849572598, 0.41977413537377745, 0.13130855447010634, -0.9471981928100199, 1.4483620520025369]\n",
      "Cost at Step 94: 0.999942187892921 - [0.44243143440100435, 0.02516712942885992, 0.17807703950393633, -0.38361331025579426, -0.01802222374436019, -2.017408483329949, 0.41345751849946555, 0.1306941043648197, -0.9435519457413387, 1.4468065523782467]\n",
      "Cost at Step 95: 0.9999272469294577 - [0.4444208506278173, 0.026220395981980546, 0.17468010507378567, -0.3842683520700742, -0.016657024441764052, -2.019816968539888, 0.40747854323415345, 0.13011475309008178, -0.9404459512825857, 1.4452886673783434]\n",
      "Cost at Step 96: 0.9999717054625975 - [0.44943802804213784, 0.02925614323625792, 0.1736036685297143, -0.3841266293584488, -0.014781403547477132, -2.018966434412149, 0.39811429236068935, 0.12931866328934305, -0.9415681024385021, 1.4421116559323892]\n",
      "Cost at Step 97: 0.9999682569165433 - [0.454232377719126, 0.03219963421191994, 0.1727908040678083, -0.3839036403893091, -0.013025589408316041, -2.017813019823688, 0.3892147040207764, 0.1285826866894829, -0.9430583901230596, 1.4390227538603888]\n",
      "Cost at Step 98: 0.9999805020026323 - [0.4585821289903246, 0.03487016401092425, 0.17205331963687173, -0.3837013300413458, -0.011432598638572369, -2.01676656579516, 0.3811404085501607, 0.12791496001729127, -0.9444104776990692, 1.4362202975963456]\n",
      "Cost at Step 99: 0.9999470550820047 - [0.4625841705409244, 0.03737066806826279, 0.17139975178107436, -0.38343813517583036, -0.009980111504484373, -2.0153402178366453, 0.37347826194035827, 0.12734506312787036, -0.9462183424822218, 1.4334358342044466]\n",
      "Cost at Step 100: 0.999950577790847 - [0.46564347973457076, 0.039597610413000583, 0.17035691529859462, -0.3828314051140976, -0.008792713041039735, -2.0115172613086996, 0.36522230839362874, 0.12716511805924843, -0.9508105176950665, 1.4296925495805806]\n",
      "Cost at Step 101: 0.9999664972463116 - [0.4677981198221847, 0.041177397093989085, 0.16852987367398825, -0.38233888010120193, -0.007966222874597343, -2.0064731946076697, 0.3570734185021932, 0.12740080482832214, -0.9569127630142463, 1.425391519373998]\n",
      "Cost at Step 102: 0.9999694259043398 - [0.46955504952551913, 0.042068344311929455, 0.1659454737612713, -0.3823067477890999, -0.007455236419547187, -2.001485884811152, 0.34951303739034806, 0.12793184055712012, -0.9631814620408324, 1.4210095982906108]\n",
      "Cost at Step 103: 0.999963423414857 - [0.4711464500122077, 0.042747063016961094, 0.1633924628539432, -0.3824036398397152, -0.007038269193943256, -1.9970046738050728, 0.3426665009085226, 0.1284704095380377, -0.9688843160367687, 1.4169747242285273]\n",
      "Cost at Step 104: 0.999945415650105 - [0.4712990860437209, 0.04232045725607377, 0.15950455178050363, -0.38301583580460835, -0.006983378183884602, -1.9933375243993592, 0.3367694174817897, 0.12932772498852504, -0.9734511579519203, 1.4131141448037678]\n",
      "Cost at Step 105: 0.9999664948498216 - [0.46966992012082487, 0.04063677538530896, 0.15405056034882997, -0.3841564178112327, -0.007259880667062152, -1.9909027254897573, 0.33189219839626194, 0.1304693677146631, -0.9762104559815498, 1.4095467300994022]\n",
      "Cost at Step 106: 0.9999634547461649 - [0.4677845112055045, 0.038612773057137104, 0.14833433871605653, -0.38551721105259135, -0.00759624823061945, -1.9893205299698011, 0.3276220501357629, 0.13160236221193014, -0.977964887217034, 1.406348568815846]\n",
      "Cost at Step 107: 0.9999734250221813 - [0.46509096830550145, 0.03642633597738486, 0.14260034051374917, -0.38667881167446494, -0.007924168073897833, -1.9881781459954226, 0.323815904239947, 0.13266089464338365, -0.9788566232027189, 1.4035307275420834]\n",
      "Cost at Step 108: 0.9999660893474521 - [0.4626853750381914, 0.0344404021549068, 0.13739390052801578, -0.3877503553128065, -0.008221294360199884, -1.9871654702970842, 0.32036411557483246, 0.13362064500140225, -0.9796569460642153, 1.4009768173510169]\n",
      "Cost at Step 109: 0.99997064757203 - [0.4597751158162047, 0.032047142683358364, 0.13156955269861523, -0.38887693033648235, -0.008319113944261472, -1.9878414875809953, 0.317017761191315, 0.1343402137086169, -0.9782215884495129, 1.3990507126614253]\n",
      "Cost at Step 110: 0.9999762676815632 - [0.4570914864643925, 0.029718061729788796, 0.1259208335448683, -0.3899536929570486, -0.008292911646519125, -1.9891443401056261, 0.3137819202293519, 0.13488774747566293, -0.9760704673167493, 1.3974849632828061]\n",
      "Cost at Step 111: 0.9999719889565832 - [0.45491050136064454, 0.02758566855865398, 0.12070771455484854, -0.3910186941570872, -0.008220522604674199, -1.990617478701752, 0.3107478175821201, 0.13533885218685504, -0.9738865349909066, 1.3961275133460571]\n",
      "Cost at Step 112: 0.9999806259583964 - [0.45312021896316723, 0.025695740245247154, 0.11563850450943419, -0.3917203639630685, -0.007770527276455969, -1.992990182203537, 0.3071261968508602, 0.13538762091967993, -0.9706365783750054, 1.3953399096489882]\n",
      "Cost at Step 113: 0.9999803247493586 - [0.4523394712887665, 0.024098861908637456, 0.1109194663331888, -0.3923776084951401, -0.007121625925808845, -1.9958411743421989, 0.3032131617803783, 0.13520529307523182, -0.9671856641922979, 1.3948831015743546]\n",
      "Cost at Step 114: 0.9999755323001125 - [0.4519634476746838, 0.022850792953146137, 0.10660429451468333, -0.39264074172296215, -0.006271405216117066, -1.9986925822458033, 0.29887283548821947, 0.13479472570806383, -0.9638141098361255, 1.39474302823319]\n",
      "Cost at Step 115: 0.9999640443673469 - [0.4516225297298295, 0.021719244056216235, 0.10269198831752985, -0.39287930875927285, -0.005500561348589721, -2.0012777818924747, 0.2949377232455877, 0.13442248902397555, -0.9607573247927954, 1.3946160321825547]\n",
      "Cost at Step 116: 0.9999787027500657 - [0.45124346685399097, 0.021114472258862833, 0.09922474468661245, -0.3921910245495407, -0.004517059582241635, -2.0030579504074217, 0.29003802543672885, 0.13381750665516587, -0.9586138384960305, 1.3948533483771992]\n",
      "Cost at Step 117: 0.9999757130193764 - [0.4508998164099739, 0.02056619884271763, 0.09608141439646772, -0.3915670405373681, -0.003625437531732843, -2.004671813791007, 0.2855960622590463, 0.13326904234165873, -0.9566705988649636, 1.3950684942487388]\n",
      "Cost at Step 118: 0.9999703598725873 - [0.45058828027216513, 0.020069161849232537, 0.09323182988223379, -0.3910013681442003, -0.0028171380088347618, -2.00613486082015, 0.2815692027448543, 0.13277183229236045, -0.9549089559864129, 1.395263534623864]\n",
      "Cost at Step 119: 0.9999778619237528 - [0.45104025542725634, 0.020046847501192275, 0.09062128192253822, -0.38997746089905466, -0.0021894028896730132, -2.006395451768586, 0.27647493866157624, 0.13240976656274323, -0.9550923434899626, 1.3957588157108587]\n",
      "Cost at Step 120: 0.9999710528226039 - [0.45168502729646737, 0.020087524323443913, 0.08821441737543705, -0.38902724190049254, -0.0016685876079334241, -2.0064762415954673, 0.2716066656443384, 0.13212350689285499, -0.955606152379772, 1.3962721698256302]\n",
      "Cost at Step 121: 0.999968767822656 - [0.45230289745003255, 0.020256410734005838, 0.08578447929197207, -0.3878866290003798, -0.0014687735493147825, -2.0056788265627263, 0.2663776140145381, 0.13210721683227908, -0.9573218598746234, 1.3969584915473254]\n",
      "Cost at Step 122: 0.9999818302536893 - [0.4527179733912407, 0.020436452118678117, 0.08295907084276793, -0.3866088026846557, -0.0018369338020419348, -2.0037805389967778, 0.26055861999499175, 0.13258724544141437, -0.9605291370206136, 1.3979429064453104]\n",
      "Cost at Step 123: 0.9999793328339263 - [0.45308633397953235, 0.020595762923918723, 0.08037474746504213, -0.38544969235435345, -0.0021850008269973156, -2.002043605276304, 0.2552635728747388, 0.13303504035988464, -0.963461870672135, 1.398846308425369]\n",
      "Cost at Step 124: 0.9999712513553441 - [0.453612400490997, 0.020615194417816535, 0.07761804467695999, -0.38460838296808464, -0.0027098062156368386, -2.0005326099965157, 0.2502534442098755, 0.13363013995599302, -0.9662351315084251, 1.399809379123168]\n",
      "Cost at Step 125: 0.9999727075504676 - [0.45368438428936175, 0.02043056365555776, 0.07460109697329392, -0.38390281916241586, -0.0033988843787994072, -1.9991718203095228, 0.24556203694583129, 0.13436480720605845, -0.9686200345557862, 1.4008460529981115]\n",
      "Cost at Step 126: 0.9999800461258854 - [0.45311285881348695, 0.02009766181316365, 0.07147582566013526, -0.3831865777360391, -0.0041459157701436876, -1.9979188441773592, 0.24124519596437569, 0.13514324677508258, -0.9705451043812554, 1.4019066923778105]\n",
      "Cost at Step 127: 0.9999794519097789 - [0.45335514534080507, 0.019466170606173634, 0.06783191225120182, -0.38321708133600224, -0.004994496107343642, -1.997698243338565, 0.23724950289473415, 0.13601044835309128, -0.9716108162893603, 1.4030675367471075]\n",
      "Cost at Step 128: 0.999978993606901 - [0.4535845068609556, 0.018783097256877993, 0.06426553401646483, -0.3833561041477786, -0.0057879193894253955, -1.9977224580653976, 0.23360255848526765, 0.13682266944026794, -0.9723106873617049, 1.4041758365219816]\n",
      "Cost at Step 129: 0.9999818398627061 - [0.45372658142493605, 0.018070784564867434, 0.06080224265854579, -0.3835321716912225, -0.006498403081895681, -1.997925017801691, 0.2302728494354441, 0.1375558835394457, -0.9726954316137838, 1.405221884276379]\n",
      "Cost at Step 130: 0.9999812232859641 - [0.45335565526608534, 0.01707850357441711, 0.05677902453788005, -0.3837147204047228, -0.006996876900527773, -1.99874302871046, 0.22713379063237402, 0.13811377595622043, -0.9720127461707129, 1.4063124070482]\n",
      "Cost at Step 131: 0.9999817626928926 - [0.45274709542706465, 0.015919191506525165, 0.05235371124908623, -0.3838712547216035, -0.007217941449985441, -2.000060570696402, 0.22408837580216281, 0.13844140925347423, -0.9705181159142083, 1.4073816108804966]\n",
      "Cost at Step 132: 0.9999822419843836 - [0.4522053114714897, 0.014811631664226145, 0.04810909914708759, -0.3840083722520991, -0.007315368385145818, -2.00142509515445, 0.2212111831651915, 0.13865917249698212, -0.9689254656173157, 1.4083595133270375]\n",
      "Cost at Step 133: 0.9999812826486758 - [0.4517143651881743, 0.013807998415442259, 0.044262775301718064, -0.3841326235172654, -0.007403653376254994, -2.002661581023243, 0.21860396856406905, 0.13885650217814915, -0.9674822598542007, 1.4092456553921808]\n",
      "Cost at Step 134: 0.9999866069187728 - [0.45237665206263256, 0.012950619250006389, 0.040332560380260965, -0.38442915714744813, -0.007100989977586045, -2.004226125734167, 0.21564873392860284, 0.13874435326754053, -0.9660080177713345, 1.4099301109320672]\n",
      "Cost at Step 135: 0.9999850883837756 - [0.4529782636211331, 0.0122129492050775, 0.03665983283451432, -0.384596055404919, -0.006706306505203964, -2.0055846304472142, 0.21275775105871514, 0.13855188084042008, -0.9647151258784641, 1.410508454976135]\n",
      "Cost at Step 136: 0.9999853012363056 - [0.45370809125445744, 0.011706198497349442, 0.033077183923409206, -0.3845097788745999, -0.006031556711095928, -2.0065407782350784, 0.209485412507581, 0.1381457984210035, -0.9639050927162048, 1.4108538376258244]\n",
      "Cost at Step 137: 0.999985108194192 - [0.4549779584958856, 0.011370794332030594, 0.02966957121940186, -0.3845003875864319, -0.005252197732084245, -2.007339057299534, 0.20605897176264262, 0.13766564647123006, -0.9635192696299504, 1.411017127801406]\n",
      "Cost at Step 138: 0.999983275024996 - [0.4562727397998631, 0.011076548508477705, 0.026565469766482722, -0.3845418808077079, -0.004537358410978361, -2.00809701110497, 0.20291912981278878, 0.13722551160751031, -0.9632042630923576, 1.411155264062923]\n",
      "Cost at Step 139: 0.9999830491459764 - [0.45751277003477575, 0.010810704204932844, 0.023750371671774677, -0.3846067034065621, -0.003889303936564386, -2.008809266676385, 0.20007147636191677, 0.13682659396122213, -0.9629221319063848, 1.4112797660551888]\n",
      "Cost at Step 140: 0.9999833627563964 - [0.4592252866618011, 0.010700845768537375, 0.0209638668304487, -0.38489273675595426, -0.003287482467651377, -2.009217911192212, 0.19695987783136318, 0.13649997534809044, -0.9633800862227945, 1.4111732226045592]\n",
      "Cost at Step 141: 0.9999853898551749 - [0.4611342850024984, 0.010625587289106498, 0.018365723936744037, -0.3853149756337008, -0.0027491950439687465, -2.0096264866852263, 0.19402053413039266, 0.13622190192014613, -0.963975486315705, 1.411019716559971]\n",
      "Cost at Step 142: 0.9999845343337446 - [0.4621934320428735, 0.010602169568473617, 0.015779152313734602, -0.3855373210237682, -0.002313057907850635, -2.0094365792778848, 0.19101311502086407, 0.13605121373060686, -0.9650861269960478, 1.4107228131123566]\n",
      "Cost at Step 143: 0.9999815262424474 - [0.46337088425167844, 0.010581428004260185, 0.013425395959250698, -0.38583202464040983, -0.0019192876493360294, -2.0093510847907052, 0.18828264017660992, 0.13589831265845817, -0.9661021926353568, 1.4104516055046294]\n",
      "Cost at Step 144: 0.9999822994420554 - [0.4643104554136029, 0.010562337649019843, 0.011262778817266261, -0.38606934902111784, -0.0015720382520355322, -2.009210927186686, 0.18578145350639663, 0.13577100698754146, -0.9670695004451343, 1.4101938989101717]\n",
      "Cost at Step 145: 0.9999873944697223 - [0.4646586268457478, 0.010512731445213813, 0.008952935793053785, -0.38637063770327196, -0.0013771898185385321, -2.008826794703605, 0.18325604426239295, 0.13579337403212427, -0.9683468184465847, 1.4098193207403458]\n",
      "Cost at Step 146: 0.9999863742127383 - [0.4649740373711151, 0.010467792896377847, 0.00686043555060427, -0.38664357687254525, -0.0012006754894251577, -2.0084788068564614, 0.18096826132626093, 0.13581363646793482, -0.9695039482264711, 1.4094799882115523]\n",
      "Cost at Step 147: 0.9999868544707448 - [0.4649275598437791, 0.010407312742255754, 0.004834727486259197, -0.3868752438397613, -0.0010841478252045602, -2.0080760803766227, 0.17881577579069477, 0.13587917744592864, -0.9706210902519947, 1.4091390890389548]\n",
      "Cost at Step 148: 0.9999882156361858 - [0.46416292318607033, 0.010298784408533697, 0.002674872249273559, -0.3870975192734513, -0.0010837080937035938, -2.007589715030885, 0.17667102027625492, 0.13605239279365824, -0.9717103032362189, 1.4087470551021066]\n",
      "Cost at Step 149: 0.9999858910876408 - [0.4635407779351966, 0.010164557728980792, 0.000439553653927349, -0.38757586063129984, -0.0011605932459026126, -2.0073733314713422, 0.1745517920345708, 0.13629264424654317, -0.97268620799266, 1.4083343458131152]\n",
      "Cost at Step 150: 0.9999879354009467 - [0.4622965935178798, 0.009990108329412632, -0.0021105946920007648, -0.3881731495091654, -0.0013367654284291736, -2.007371589024346, 0.1722383982756266, 0.13663045840924615, -0.9734293122269438, 1.4078706721229501]\n",
      "Cost at Step 151: 0.999983491781044 - [0.46063285698583994, 0.009827329591738782, -0.004614159334345872, -0.3886266855802173, -0.001508034175411433, -2.0073540579594282, 0.16994760577331344, 0.13695571369013496, -0.9740121654891993, 1.407424806973364]\n",
      "Cost at Step 152: 0.9999857075326979 - [0.4594681833848455, 0.009683512758137964, -0.00692288626016698, -0.3891914717065696, -0.0016588483915738789, -2.007513712904253, 0.1678296051977076, 0.13724786527517382, -0.9745205813331846, 1.4070144906378586]\n",
      "Cost at Step 153: 0.999986281561508 - [0.45772440187236857, 0.009616274341496173, -0.00927151679733392, -0.38944033348484436, -0.0016974479131881247, -2.007707435835786, 0.16544157949801402, 0.1374358327181225, -0.9747423467316059, 1.4066264236116548]\n",
      "Cost at Step 154: 0.9999888335669551 - [0.4562664637323554, 0.009624453099393148, -0.011568895948385394, -0.38965281843631794, -0.0016348352850487653, -2.0081172694980474, 0.16289535821933024, 0.13752660698270736, -0.9747729705817745, 1.4062487041867275]\n",
      "Cost at Step 155: 0.9999852589783018 - [0.45447611786599224, 0.00972859522493394, -0.01382688331489175, -0.3894986611372351, -0.001443456548920159, -2.008468039522421, 0.16007969239994022, 0.13749615864682008, -0.9746151599268656, 1.4058962344815908]\n",
      "Cost at Step 156: 0.9999869033279376 - [0.4531879678804781, 0.009826539980575009, -0.015886425423082967, -0.3894894852220665, -0.0012660429386019481, -2.0089276202083566, 0.1575107368843273, 0.13746495599128192, -0.974468164552102, 1.405578098983236]\n",
      "Cost at Step 157: 0.9999888935484976 - [0.4525580486040868, 0.009941415664955903, -0.01781799583269116, -0.3896315845507607, -0.0010776750683127736, -2.0095677874879714, 0.15503204358986714, 0.13741333949762044, -0.9743171182729589, 1.4052865041866403]\n",
      "Cost at Step 158: 0.9999916871066825 - [0.453355818784208, 0.01010352425537888, -0.019778711733535143, -0.3900773530301286, -0.0008311735806995136, -2.01064222546282, 0.1523357833334313, 0.13730403403206634, -0.9741938964421906, 1.4049982055611971]\n",
      "Cost at Step 159: 0.9999885744111697 - [0.45445701139209543, 0.010249967330445309, -0.021787480328104458, -0.390413228233351, -0.0005390292194196082, -2.0116652657814704, 0.14954321689528216, 0.13714761854820284, -0.974207398072544, 1.404712621126472]\n",
      "Cost at Step 160: 0.9999887485281815 - [0.4558420711614759, 0.010303929423463906, -0.024000331036954526, -0.39062391974739635, -0.0001991357289343505, -2.012590990126372, 0.1466541680461645, 0.13694318879871242, -0.974515691971151, 1.4044137524667457]\n",
      "Cost at Step 161: 0.9999902839969312 - [0.45652926754302686, 0.010238700598934355, -0.026344691204948083, -0.39045252204784087, 0.0001387551743881224, -2.0131019252993685, 0.1438420542532065, 0.13673286762334724, -0.9750727634621303, 1.4041248730321194]\n",
      "Cost at Step 162: 0.9999883178379426 - [0.456891028843333, 0.009996606199465583, -0.028950465845708703, -0.39011690796886406, 0.00042599473422720737, -2.013372065052211, 0.1411021233178115, 0.1365614020487122, -0.9759175988082467, 1.4038290967121525]\n",
      "Cost at Step 163: 0.9999907076459431 - [0.45677957001098124, 0.009638881702257563, -0.03166054007837512, -0.38964772478332027, 0.0006252369023978498, -2.013428707817153, 0.13849013169648708, 0.1364613302398857, -0.9768712211456072, 1.4035478984880059]\n",
      "Cost at Step 164: 0.9999876945717082 - [0.45672528384268574, 0.009309143518055157, -0.034132716901012224, -0.38924560016789067, 0.0008005356350054418, -2.0135029964434734, 0.13611770982615887, 0.13637503771297502, -0.9777407026838335, 1.4032951477360265]\n",
      "Cost at Step 165: 0.9999880817721781 - [0.4563822568846567, 0.008984956171987623, -0.03644287977510048, -0.3887906049061944, 0.0009241534014896797, -2.0134765408434285, 0.1339308213901171, 0.13632780727044158, -0.9785350566613712, 1.4030703139380145]\n",
      "Cost at Step 166: 0.9999878987899615 - [0.45633315148489106, 0.008683620171007024, -0.03856785218331879, -0.3885020338206008, 0.0010194479586314884, -2.0135748898674937, 0.13193375190934276, 0.13629964470693606, -0.9792493876745918, 1.402868906113563]\n",
      "Cost at Step 167: 0.9999903727084494 - [0.45628868240350756, 0.008410735003323563, -0.04049219394701635, -0.388240708362693, 0.0011057452179152397, -2.0136639532032627, 0.1301252370352382, 0.13627414113020392, -0.9798962746422581, 1.4026865143486227]\n",
      "Cost at Step 168: 0.9999880960337961 - [0.4566090262900368, 0.008152312577253996, -0.042296174690323436, -0.3882035129693109, 0.0011347912334121566, -2.0139423227680036, 0.12844264367684896, 0.13629431194591762, -0.980442653502329, 1.4025277853011804]\n",
      "Cost at Step 169: 0.9999868272283047 - [0.4567835351431635, 0.007881149752713838, -0.04412094281997745, -0.3883350950911825, 0.0009654676401362655, -2.0143540428726117, 0.12673891166564777, 0.1364858883125279, -0.9807495088629903, 1.4024068473792761]\n",
      "Cost at Step 170: 0.9999915972343406 - [0.45646889797801343, 0.007625074974905607, -0.04583142557786037, -0.38833289733683557, 0.0007470269767201702, -2.014612926533241, 0.12512470342000215, 0.13671682757621006, -0.9809577833104933, 1.402309570626071]\n",
      "Cost at Step 171: 0.9999910062287994 - [0.4562889448657948, 0.007391112368010303, -0.04741366642193884, -0.38840463773181155, 0.0005219760481421369, -2.0149280839686283, 0.12362441959493631, 0.13694983622969248, -0.9811150434201775, 1.4022260100994162]\n",
      "Cost at Step 172: 0.999989828678656 - [0.45645617340797956, 0.0071798388044060175, -0.04896717802980008, -0.3887086874411378, 0.00022608855123984305, -2.0154846034098237, 0.1220919001824916, 0.1372428693532047, -0.9811239010331197, 1.4021562833030201]\n",
      "Cost at Step 173: 0.9999875762064164 - [0.4566076004676726, 0.006988528551083046, -0.05037389780101376, -0.3889840073885751, -4.184043419615805e-05, -2.0159885371318618, 0.12070418904688483, 0.13750821367657046, -0.9811319216869563, 1.402093145018553]\n",
      "Cost at Step 174: 0.9999876252374504 - [0.4565195649945639, 0.006823159912106224, -0.0517500411006047, -0.38917085167406207, -0.00030393845797323596, -2.0164406086494577, 0.11925497693730638, 0.13776730345695595, -0.9810638514478598, 1.4020253373420926]\n",
      "Cost at Step 175: 0.9999897237438046 - [0.45563059604671424, 0.00668579960888799, -0.053101579836964605, -0.38899888290854095, -0.00052440593322362, -2.0165951940548457, 0.11770410548901358, 0.1379890025580823, -0.9809452563091868, 1.401954257013909]\n",
      "Cost at Step 176: 0.9999914908473431 - [0.4544526556878244, 0.006590520949737291, -0.054457203128808536, -0.38862929629176446, -0.0006752523807297946, -2.0166374675311762, 0.11597965615034303, 0.1381493776100243, -0.9807954333510883, 1.4018738514756384]\n",
      "Cost at Step 177: 0.9999904790614826 - [0.453555771736479, 0.00653814904077198, -0.055802515024507754, -0.3882816968014771, -0.0007643259793391315, -2.016756629454966, 0.11412263334636341, 0.13825551372708728, -0.9806374833227818, 1.4017848781701425]\n",
      "Cost at Step 178: 0.999990721496238 - [0.4538696787234251, 0.006518563460128339, -0.05715876476883842, -0.3883354386013052, -0.0008011600574765169, -2.017288640360914, 0.11215572987513374, 0.13831790621467802, -0.9804844845959003, 1.4016703224133458]\n",
      "Cost at Step 179: 0.9999916486599748 - [0.4545978965289333, 0.006499252260406285, -0.05845472235074909, -0.38853029900144825, -0.000812817084951165, -2.0179309774670613, 0.11028605826943617, 0.1383570787236929, -0.9803739657868111, 1.4015499926945558]\n",
      "Cost at Step 180: 0.999990238337642 - [0.45499709061123983, 0.006472701843992152, -0.059664315382336644, -0.3885806302468714, -0.0008085955161548839, -2.018392621867338, 0.10855534983731788, 0.13838032219415491, -0.9803040444422997, 1.4014349641493746]\n",
      "Cost at Step 181: 0.9999925101980447 - [0.45512986488940677, 0.006386865296594266, -0.06094869599053177, -0.38846375395302957, -0.0007408204799835328, -2.0186419343368645, 0.10687164248125222, 0.13834920721672853, -0.9803948701730373, 1.4012946349707596]\n",
      "Cost at Step 182: 0.9999915167886397 - [0.4552807472580121, 0.00629697356221828, -0.062147284806401325, -0.3883625643969754, -0.0006707724000050964, -2.018871224944057, 0.10533206745023108, 0.1383137799670635, -0.9805066329338015, 1.4011639536291594]\n",
      "Cost at Step 183: 0.9999899132983584 - [0.45548775433255356, 0.0062150043553109435, -0.06323557199796993, -0.3882998399336514, -0.0006070714141186986, -2.019107691390784, 0.10393765119261889, 0.13828144440954315, -0.980609029799564, 1.4010458197880378]\n",
      "Cost at Step 184: 0.9999871868156509 - [0.45575577581873905, 0.006113020346312852, -0.06429036240844796, -0.38827251411915337, -0.000539633096649854, -2.019339337762711, 0.10265983258621164, 0.13824461219733752, -0.980756217538255, 1.4009291215150992]\n",
      "Cost at Step 185: 0.9999907169514173 - [0.45599843662142964, 0.006020686250136181, -0.06524534662705589, -0.38824777392325116, -0.0004785759162080768, -2.0195490653285133, 0.10150292350607412, 0.13821126511638304, -0.9808894781000363, 1.4008234654515364]\n",
      "Cost at Step 186: 0.999990012177215 - [0.4564541984606934, 0.005933974630228084, -0.06612223379375974, -0.3883233472005084, -0.0004226987958317355, -2.0198355368308016, 0.10045482575305488, 0.13818057251045412, -0.9810157029329685, 1.4007275483633972]\n",
      "Cost at Step 187: 0.9999909338689724 - [0.45671932876071725, 0.005824656895044117, -0.06697889441500388, -0.3883385455504111, -0.00036754784299409523, -2.0200233183391436, 0.09950378461602888, 0.13814988447601792, -0.981181144484456, 1.4006302343704677]\n",
      "Cost at Step 188: 0.9999902833343749 - [0.45643763862559195, 0.005483848591574148, -0.06824250297156753, -0.38822031518601796, -0.0002942596495881259, -2.0198903777459245, 0.09864070636055475, 0.13811115999370505, -0.9816849398122902, 1.4004602319886108]\n",
      "Cost at Step 189: 0.9999874627162509 - [0.4552881190240018, 0.00497151040778825, -0.06984433830530064, -0.38785187589149034, -0.0002305886005309305, -2.0193535915668765, 0.09778568786432329, 0.1380877948893583, -0.9824282417194167, 1.4002311538119867]\n",
      "Cost at Step 190: 0.999990549292023 - [0.4534175083988571, 0.004492303553063817, -0.07134175286352389, -0.3872019249129475, -0.0001867962981907965, -2.0185394220268784, 0.09696322495344592, 0.13807989385963224, -0.98310660048365, 1.4000272900021884]\n",
      "Cost at Step 191: 0.9999906572515467 - [0.45181700596172586, 0.0040763304446160215, -0.072728638598515, -0.38667706835781274, -0.00017762202236299134, -2.017867300895803, 0.0961276805101716, 0.13810020972833134, -0.983675116866478, 1.3998493455088867]\n",
      "Cost at Step 192: 0.9999882139616453 - [0.4501934782514914, 0.0037457931591570945, -0.07397073040784787, -0.3861543199233467, -0.00021395423788247173, -2.017219555887, 0.095246073496427, 0.13815884189377256, -0.9840804711760134, 1.399700352542498]\n",
      "Cost at Step 193: 0.9999912793360852 - [0.44929744168861807, 0.003480656788604508, -0.07508802916329915, -0.3859274141860322, -0.00027321938631056167, -2.016888296404176, 0.09437327370129066, 0.138235482114451, -0.9843723319327218, 1.3995736887678825]\n",
      "Cost at Step 194: 0.9999894299745655 - [0.44865447187021684, 0.0035885198520813765, -0.07584960147427522, -0.38585415463854206, -0.0005643586374080441, -2.016839173415085, 0.09288205377226487, 0.13851593628604758, -0.9838612855858409, 1.3995679233547105]\n",
      "Cost at Step 195: 0.9999891780733877 - [0.44865991184173776, 0.003723847949719529, -0.07652750323175855, -0.3860277731734104, -0.000848369119751579, -2.0170528195832547, 0.09145264484837548, 0.1387879078640438, -0.9833179173647336, 1.399574588673038]\n",
      "Cost at Step 196: 0.9999889782271272 - [0.44866483651398015, 0.0038463571446887765, -0.07714119100221789, -0.38618494575700213, -0.001105476814733897, -2.0172462281951833, 0.0901586361428137, 0.1390341170360378, -0.9828260195107579, 1.3995806226212506]\n",
      "Cost at Step 197: 0.9999883760097149 - [0.44868877649462496, 0.004167649160199381, -0.0776213658045718, -0.38614748039970653, -0.001349377271821559, -2.0174067413207624, 0.08840674563768507, 0.13926975684544476, -0.9818212414981387, 1.3996775924755227]\n",
      "Cost at Step 198: 0.9999918431084317 - [0.4481462071621093, 0.0044832443821114076, -0.07808216131969656, -0.3857941364849627, -0.0015483057088097752, -2.017287233931407, 0.0866632668197349, 0.1394644534850294, -0.9807757970361501, 1.3998141703119038]\n",
      "Cost at Step 199: 0.9999902691698804 - [0.44745934527354037, 0.004761030438634435, -0.07852658965668197, -0.38536334008929535, -0.0017180095886130877, -2.0170769697955664, 0.08505459633408101, 0.13963192989131076, -0.9798128627512378, 1.3999513877004826]\n",
      "[0.44758897046212653, 0.004850125100078896, -0.07928851023595712, -0.38505020809151536, -0.0017866018819610314, -2.016893759099315, 0.0834915721142961, 0.1397187592224067, -0.9788946802439161, 1.4001147969698071]\n"
     ]
    }
   ],
   "source": [
    "# Defines the optimization method\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "optimizer = qml.AdamOptimizer(stepsize=0.5)\n",
    "steps = 100\n",
    "\n",
    "qgrnn_params = list([random.randint(-20, 20)/10 for i in range(0, 10)])\n",
    "\n",
    "# Executes the optimization method\n",
    "\n",
    "for i in range(0, steps):\n",
    "    qgrnn_params = optimizer.step(cost_function, qgrnn_params)\n",
    "\n",
    "print(qgrnn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the Hamiltonian corresponding to the learned parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.38614954  1.          1.          0.          1.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.          0.00955861  0.          1.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.          0.          5.36582751  1.          0.          0.\n",
      "   1.          0.          0.          0.          1.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          1.          1.         -1.30603938  0.          0.\n",
      "   0.          1.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.          0.          0.          0.         -1.78709138  1.\n",
      "   1.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.          1.          0.          0.          1.         -0.39852963\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.          0.          1.          0.          1.          0.\n",
      "   3.42468484  1.          0.          0.          0.          0.\n",
      "   0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.          1.          0.          1.\n",
      "   1.         -3.25432846  0.          0.          0.          0.\n",
      "   0.          0.          0.          1.        ]\n",
      " [ 1.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -2.29943385  1.          1.          0.\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          1.         -1.22087974  0.          1.\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          1.          0.          4.47194369  1.\n",
      "   0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.          1.          0.          0.\n",
      "   0.          0.          0.          1.          1.         -2.51707723\n",
      "   0.          0.          0.          1.        ]\n",
      " [ 0.          0.          0.          0.          1.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "  -0.91001981  1.          1.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.\n",
      "   1.          0.16138789  0.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          1.          0.\n",
      "   1.          0.          4.3211569   1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          1.\n",
      "   0.          1.          1.         -2.67501043]]\n"
     ]
    }
   ],
   "source": [
    "new_ham_matrix = create_hamiltonian_matrix(qubit_number, nx.complete_graph(qubit_number), [qgrnn_params[0:6], qgrnn_params[6:10]])\n",
    "print(new_ham_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can comapre the diagonal elements of the learned Hamiltonian (bottom row) to the those of the target Hamiltonian (top row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_density_plot([np.diag(ham_matrix), np.diag(new_ham_matrix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity of colours indicates that the QGRNN was able to learn the parameters to a very high degree accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Verdon, G., McCourt, T., Luzhnica, E., Singh, V., Leichenauer, S., & Hidary, J. (2019). Quantum Graph Neural Networks. arXiv preprint [arXiv:1909.12264](https://arxiv.org/abs/1909.12264)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
